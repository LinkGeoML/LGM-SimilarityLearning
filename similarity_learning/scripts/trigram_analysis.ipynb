{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from more_itertools import windowed\n",
    "from tqdm import tqdm\n",
    "import editdistance\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(text: str, n: int = 3, step=1) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "    :param text:\n",
    "    :param n:\n",
    "    :param step:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    output = []\n",
    "\n",
    "    # split the sentence in tokens.\n",
    "    tokens = text.split()\n",
    "\n",
    "    # if only one token, then we only have BOS and EOS tags\n",
    "    if len(tokens) == 1:\n",
    "\n",
    "        chars = ['<BOS>'] + list(text) + ['<EOS>']\n",
    "        text = ' '.join(\n",
    "            [''.join(t) for t in windowed(seq=chars, n=3, step=1)])\n",
    "        output.append(text)\n",
    "\n",
    "    # We have more than 1 tokens. So we need 3 kind of tags:\n",
    "    # BOS: beginning of sentence\n",
    "    # IOS: inside of sentence\n",
    "    # EOS: end of sentence\n",
    "    else:\n",
    "        # extracting the first token, a list of the inside tokens, and the\n",
    "        # last token. We handle each one differently\n",
    "        first, *inside, last = tokens\n",
    "\n",
    "        # in the first token we put BOS tag in the beginning of the token\n",
    "        # and IOS at the end, since the sentence is not over.\n",
    "        # We also split to first token to it's characters, so we can get\n",
    "        # the n-grams.\n",
    "        first_chars = ['<BOS>'] + list(first) + ['<IOS>']\n",
    "\n",
    "        # create the n-gram texts and join them back together with a ' '\n",
    "        first_chars = ' '.join(\n",
    "            [''.join(t)\n",
    "             for t in windowed(seq=first_chars, n=n, step=step)])\n",
    "\n",
    "        # append the \"n-gramed\" token to the output list\n",
    "        output.append(first_chars)\n",
    "\n",
    "        for ins_token in inside:\n",
    "            # for each of the inside tokens use only the IOS tags\n",
    "            # we do the same procedure as in the first token.\n",
    "            inside_chars = ['<IOS>'] + list(ins_token) + ['<IOS>']\n",
    "\n",
    "            inside_chars = ' '.join(\n",
    "                [''.join(t) for t in\n",
    "                 windowed(seq=inside_chars, n=n, step=step)])\n",
    "\n",
    "            output.append(inside_chars)\n",
    "\n",
    "        # for the last token we use IOS and EOS tags.\n",
    "        # Same procedure as before.\n",
    "        last_chars = ['<IOS>'] + list(last) + ['<EOS>']\n",
    "\n",
    "        last_chars = ' '.join(\n",
    "            [''.join(t) for t in windowed(seq=last_chars, n=3, step=1)])\n",
    "\n",
    "        output.append(last_chars)\n",
    "\n",
    "    return ' '.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/toponym_distances.csv', \n",
    "                 nrows=200_000, \n",
    "                 usecols=['gid','anchor','alternate'])\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len_anchor'] = df['anchor'].str.len()\n",
    "df['len_alternate'] = df['alternate'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['anchor_3grams'] = df['anchor'].progress_apply(get_ngrams)\n",
    "df['alternate_3grams'] = df['alternate'].progress_apply(get_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['1gram_dist'] = df.progress_apply(lambda row: editdistance.eval(row['anchor'], row['alternate']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['3gram_dist'] = df.progress_apply(lambda row: editdistance.eval(row['anchor_3grams'].split(), \n",
    "                                                                   row['alternate_3grams'].split()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_texts_ngrams = pd.concat((df['anchor_3grams'], df['alternate_3grams'])).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len_anchor_3grams'] = df['anchor_3grams'].str.split().apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tokenizer_distances(texts,\n",
    "                                  data,\n",
    "                                  num_words = 10000):\n",
    "    \n",
    "    tokenizer = Tokenizer(\n",
    "        filters='', \n",
    "        lower=True,\n",
    "        split=' ',\n",
    "        char_level=False,\n",
    "        num_words=num_words,\n",
    "        oov_token='<OOV>')\n",
    "\n",
    "    tokenizer.fit_on_texts(texts=texts)    \n",
    "#     print(len(tokenizer.word_index))\n",
    "#     print(tokenizer.num_words)\n",
    "    res = pd.DataFrame()\n",
    "    res['anchor_seqs'] = pd.Series(tokenizer.texts_to_sequences(data['anchor_3grams']))\n",
    "    res['alternate_seqs'] = pd.Series(tokenizer.texts_to_sequences(data['alternate_3grams']))\n",
    "\n",
    "    distances = res.progress_apply(lambda row: editdistance.eval(row['anchor_seqs'], row['alternate_seqs']), axis=1)\n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trigram_10k_dist'] = calculate_tokenizer_distances(all_texts_ngrams, df, 10_000)\n",
    "df['trigram_20k_dist'] = calculate_tokenizer_distances(all_texts_ngrams, df, 20_000)\n",
    "df['trigram_25k_dist'] = calculate_tokenizer_distances(all_texts_ngrams, df, 25_000)\n",
    "df['trigram_50k_dist'] = calculate_tokenizer_distances(all_texts_ngrams, df, 50_000)\n",
    "df['trigram_75k_dist'] = calculate_tokenizer_distances(all_texts_ngrams, df, 75_000)\n",
    "df['trigram_100k_dist'] = calculate_tokenizer_distances(all_texts_ngrams, df, 100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['3gram_dist'] != df['trigram_10k_dist']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
